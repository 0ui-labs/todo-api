#!/usr/bin/env python3
"""
AI-Powered Task Analyzer Hook for Claude Code
Uses Claude API or local LLM to intelligently analyze tasks and determine
required documentation and technologies.
"""

import os
import sys
import json
import requests
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class TaskAnalysis:
    """Result of AI task analysis"""
    task_type: str  # programming, planning, mixed
    technologies: List[Dict[str, str]]  # [{name, library_id, topics}]
    guides_needed: List[str]  # ['python-guide', 'prp-framework']
    confidence: float
    reasoning: str

class AITaskAnalyzer:
    """Analyzes tasks using AI for intelligent context loading"""
    
    def __init__(self):
        # Try to get API key from environment
        self.claude_api_key = os.getenv("ANTHROPIC_API_KEY")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.local_llm_url = os.getenv("LOCAL_LLM_URL", "http://localhost:11434/api/generate")
        
    def analyze_with_claude(self, task: str) -> Optional[TaskAnalysis]:
        """Use Claude API to analyze the task"""
        if not self.claude_api_key:
            return None
            
        headers = {
            "x-api-key": self.claude_api_key,
            "anthropic-version": "2023-06-01",
            "content-type": "application/json"
        }
        
        prompt = self._create_analysis_prompt(task)
        
        data = {
            "model": "claude-3-haiku-20240307",  # Fast and cheap for analysis
            "max_tokens": 500,
            "messages": [{
                "role": "user",
                "content": prompt
            }]
        }
        
        try:
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers=headers,
                json=data
            )
            
            if response.status_code == 200:
                result = response.json()
                return self._parse_ai_response(result["content"][0]["text"])
        except Exception as e:
            print(f"Claude API error: {e}", file=sys.stderr)
            
        return None
    
    def analyze_with_openai(self, task: str) -> Optional[TaskAnalysis]:
        """Use OpenAI API to analyze the task"""
        if not self.openai_api_key:
            return None
            
        headers = {
            "Authorization": f"Bearer {self.openai_api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = self._create_analysis_prompt(task)
        
        data = {
            "model": "gpt-3.5-turbo",  # Fast and efficient
            "messages": [{
                "role": "user",
                "content": prompt
            }],
            "max_tokens": 500,
            "temperature": 0.3  # More deterministic
        }
        
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data
            )
            
            if response.status_code == 200:
                result = response.json()
                return self._parse_ai_response(result["choices"][0]["message"]["content"])
        except Exception as e:
            print(f"OpenAI API error: {e}", file=sys.stderr)
            
        return None
    
    def analyze_with_local_llm(self, task: str) -> Optional[TaskAnalysis]:
        """Use local LLM (Ollama) to analyze the task"""
        prompt = self._create_analysis_prompt(task)
        
        data = {
            "model": "llama2",  # or any model you have locally
            "prompt": prompt,
            "stream": False,
            "format": "json"
        }
        
        try:
            response = requests.post(self.local_llm_url, json=data)
            
            if response.status_code == 200:
                result = response.json()
                return self._parse_ai_response(result["response"])
        except Exception as e:
            print(f"Local LLM error: {e}", file=sys.stderr)
            
        return None
    
    def _create_analysis_prompt(self, task: str) -> str:
        """Create the prompt for AI analysis"""
        return f"""Analyze this software development task and provide a JSON response:

Task: "{task}"

Determine:
1. Task type: Is this primarily "programming", "planning", or "mixed"?
2. Technologies: What specific technologies/libraries are mentioned or implied?
3. Documentation needed: Should we load python-guide, prp-framework, or both?

For technologies, identify from this list:
- FastAPI (web framework) -> /tiangolo/fastapi
- SQLAlchemy (ORM) -> /sqlalchemy/sqlalchemy  
- PostgreSQL (database) -> /postgresql/postgresql
- Redis (caching) -> /redis/redis
- Pydantic (validation) -> /pydantic/pydantic
- pytest (testing) -> /pytest-dev/pytest
- JWT/python-jose (auth) -> /python-jose/python-jose

Respond ONLY with JSON in this format:
{{
  "task_type": "programming|planning|mixed",
  "technologies": [
    {{
      "name": "technology name",
      "library_id": "/org/library",
      "topics": ["relevant", "topics"]
    }}
  ],
  "guides_needed": ["python-guide", "prp-framework"],
  "confidence": 0.9,
  "reasoning": "Brief explanation"
}}"""
    
    def _parse_ai_response(self, response: str) -> Optional[TaskAnalysis]:
        """Parse AI response into TaskAnalysis object"""
        try:
            # Extract JSON from response (in case there's extra text)
            start = response.find('{')
            end = response.rfind('}') + 1
            if start >= 0 and end > start:
                json_str = response[start:end]
                data = json.loads(json_str)
                
                return TaskAnalysis(
                    task_type=data.get("task_type", "programming"),
                    technologies=data.get("technologies", []),
                    guides_needed=data.get("guides_needed", []),
                    confidence=data.get("confidence", 0.5),
                    reasoning=data.get("reasoning", "")
                )
        except Exception as e:
            print(f"Failed to parse AI response: {e}", file=sys.stderr)
            
        return None
    
    def analyze(self, task: str) -> TaskAnalysis:
        """Analyze task using available AI services"""
        # Try each AI service in order of preference
        result = (
            self.analyze_with_claude(task) or
            self.analyze_with_openai(task) or
            self.analyze_with_local_llm(task)
        )
        
        if result and result.confidence > 0.7:
            return result
        
        # Fallback to rule-based analysis
        return self._fallback_analysis(task)
    
    def _fallback_analysis(self, task: str) -> TaskAnalysis:
        """Simple rule-based fallback"""
        task_lower = task.lower()
        
        # Basic classification
        if any(word in task_lower for word in ["implement", "code", "fix", "debug"]):
            task_type = "programming"
        elif any(word in task_lower for word in ["plan", "design", "architect"]):
            task_type = "planning"
        else:
            task_type = "mixed"
        
        # Basic technology detection
        technologies = []
        if "api" in task_lower or "endpoint" in task_lower:
            technologies.append({
                "name": "FastAPI",
                "library_id": "/tiangolo/fastapi",
                "topics": ["routing", "dependencies"]
            })
        
        if "test" in task_lower:
            technologies.append({
                "name": "pytest",
                "library_id": "/pytest-dev/pytest",
                "topics": ["fixtures", "testing"]
            })
        
        # Guide detection
        guides = []
        if task_type in ["programming", "mixed"]:
            guides.append("python-guide")
        if task_type in ["planning", "mixed"] or "/prp" in task:
            guides.append("prp-framework")
        
        return TaskAnalysis(
            task_type=task_type,
            technologies=technologies,
            guides_needed=guides,
            confidence=0.5,
            reasoning="Fallback rule-based analysis"
        )

def generate_instructions(analysis: TaskAnalysis) -> str:
    """Generate instructions based on AI analysis"""
    instructions = []
    
    # Header based on confidence
    if analysis.confidence > 0.8:
        header = f"ðŸ¤– AI TASK ANALYSIS (confidence: {analysis.confidence:.0%}):\n"
    else:
        header = f"ðŸ“Š TASK ANALYSIS (confidence: {analysis.confidence:.0%}):\n"
    
    instructions.append(header)
    
    # Task type
    task_emoji = {"programming": "ðŸ”§", "planning": "ðŸ“‹", "mixed": "ðŸ”€"}.get(analysis.task_type, "â“")
    instructions.append(f"{task_emoji} Task Type: {analysis.task_type.upper()}")
    
    if analysis.reasoning:
        instructions.append(f"ðŸ’­ Analysis: {analysis.reasoning}\n")
    
    # Technologies to load
    if analysis.technologies:
        instructions.append("ðŸ“š Load Context7 Documentation for:")
        for tech in analysis.technologies:
            topics_str = f" (focus: {', '.join(tech['topics'])})" if tech.get('topics') else ""
            instructions.append(f"  - {tech['name']}: `{tech['library_id']}`{topics_str}")
        instructions.append("")
    
    # Guides to consult
    if analysis.guides_needed:
        instructions.append("ðŸ“– Consult Project Guides:")
        if "python-guide" in analysis.guides_needed:
            instructions.append("  - docs/PYTHON-GUIDE.md (coding standards)")
        if "prp-framework" in analysis.guides_needed:
            instructions.append("  - docs/PRP-FRAMEWORK.md (planning structure)")
    
    return "\n".join(instructions)

def main():
    """Main entry point"""
    if len(sys.argv) > 1:
        task = " ".join(sys.argv[1:])
    else:
        task = sys.stdin.read().strip()
    
    if not task:
        return
    
    # Analyze task with AI
    analyzer = AITaskAnalyzer()
    analysis = analyzer.analyze(task)
    
    # Generate and print instructions
    instructions = generate_instructions(analysis)
    if instructions:
        print(instructions)
        print("-" * 60)
    
    # Always output the original task
    print(task)

if __name__ == "__main__":
    main()